{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Fri Jan 17 17:03:35 2019  \n",
    "Group 9  \n",
    "@authors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the sentence generation with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_gen(model: KeyedVectors,\n",
    "             word_list: list,\n",
    "             up: bool = False,\n",
    "             cap: bool = False,\n",
    "             same: bool = False,\n",
    "             low: bool = False) -> set:\n",
    "\n",
    "    def my_capitalize(w: str) -> str:\n",
    "        return \"_\".join(list(map(lambda x: x.capitalize(), w.split('_'))))\n",
    "\n",
    "    l_upper, l_lower, l_cap, l_same = [], [], [], []\n",
    "    if up:\n",
    "        l_upper = [m.upper() for m in word_list\n",
    "                   if m.upper() in model.vocab]\n",
    "    if cap:\n",
    "        l_cap = [my_capitalize(m)\n",
    "                 for m in word_list\n",
    "                 if my_capitalize(m) in model.vocab]\n",
    "    if same:\n",
    "        l_same = [m for m in word_list\n",
    "                  if m in model.vocab]\n",
    "    if low:\n",
    "        l_lower = [m.lower() for m in word_list\n",
    "                   if m.lower() in model.vocab]\n",
    "\n",
    "    return set(l_upper + l_cap + l_lower + l_same)\n",
    "\n",
    "# init the tags\n",
    "\n",
    "\n",
    "def init_sent(list_words: list) -> list:\n",
    "    res = []\n",
    "    for e in list_words:\n",
    "        if e.startswith('#'):\n",
    "            res.append((e, '#'))\n",
    "        elif e.startswith('$'):\n",
    "            res.append((e, '#'))\n",
    "        else:\n",
    "            for w in e.split():\n",
    "                res.append((w, CT_TAG_O))\n",
    "    return res\n",
    "\n",
    "# initialize a structure\n",
    "# if tuple : choose one at random\n",
    "\n",
    "\n",
    "def init_structure(struct) -> list:\n",
    "    res = []\n",
    "    if isinstance(struct, str):\n",
    "        struct = struct.strip()\n",
    "        if len(struct):\n",
    "            res += [struct]\n",
    "    elif isinstance(struct, tuple):\n",
    "        n = np.random.choice(len(struct), 1)[0]\n",
    "        l = list(struct)[n]\n",
    "        res += init_structure(l)\n",
    "    elif isinstance(struct, list):\n",
    "        for s in struct:\n",
    "            if isinstance(s, str):\n",
    "                s = s.strip()\n",
    "                if len(s):\n",
    "                    res += [s]\n",
    "            elif isinstance(s, tuple):\n",
    "                n = np.random.choice(len(s), 1)[0]\n",
    "                l = list(s)[n]\n",
    "                res += init_structure(l)\n",
    "            elif isinstance(s, list):\n",
    "                n = np.random.choice(len(s), 1)[0]\n",
    "                l = list(s)[n]\n",
    "                res += init_structure(l)\n",
    "    return res\n",
    "\n",
    "# make the swaps\n",
    "\n",
    "\n",
    "def swap(tagger: dict, gen: dict, voc: dict, W: tuple):\n",
    "    def wt_augm(w, tag):\n",
    "        ws = w.split(\"_\")\n",
    "        ws = [w for w in ws if w != '']\n",
    "        t = tag.split(CT_SEP)\n",
    "        t_name = t[0]\n",
    "        if len(t) > 1:\n",
    "            t_suffix = t[-1]\n",
    "        else:\n",
    "            t_suffix = CT_SUF_B\n",
    "\n",
    "        tags = [t_name+CT_SEP+t_suffix if i == 0\n",
    "                else t_name+CT_SEP+CT_SUF_E if i+1 == len(ws)\n",
    "                else t_name+CT_SEP+CT_SUF_I\n",
    "                for i in range(len(ws))]\n",
    "        tags = [CT_TAG_O if t.startswith(CT_TAG_O)\n",
    "                else t\n",
    "                for t in tags]\n",
    "        return list(zip(ws, tags))\n",
    "\n",
    "    hw, tag = W\n",
    "    if hw.startswith('#'):\n",
    "        n = random.sample(range(len(tagger[hw])), 1)[0]\n",
    "        w = tagger[hw][n]\n",
    "        w = list(zip(w[0].split(), w[1].split()))\n",
    "        w = [swap(tagger, gen, voc, (m[0], tag)) if (tag != '#')\n",
    "             else swap(tagger, gen, voc, (m[0], m[1]))\n",
    "             for m in w]\n",
    "        if len(w) == 1:\n",
    "            w = w[0]\n",
    "\n",
    "    elif hw.startswith('$GEN$'):\n",
    "        key = hw.split('$GEN$')[1]\n",
    "        a = select_from_sentence_gen(gen, key)[1:]\n",
    "        w = wt_augm(a, tag)\n",
    "\n",
    "    elif hw.startswith('$VOC$'):\n",
    "        _, key, sub_key = hw.split('$VOC$')[1].split('#')\n",
    "        words_set = voc[key][sub_key]\n",
    "        n = random.sample(range(len(words_set)), 1)[0]\n",
    "        w = list(words_set)[n]\n",
    "        w = wt_augm(w, tag)\n",
    "    else:\n",
    "        w = (hw, tag)\n",
    "\n",
    "    return w\n",
    "\n",
    "# remove list of list\n",
    "\n",
    "\n",
    "def remove_lists(listsOfLists: list) -> list:\n",
    "    continue_loop = True\n",
    "    while continue_loop:\n",
    "        res = []\n",
    "        continue_loop = False\n",
    "        for t in listsOfLists:\n",
    "            if isinstance(t, list):\n",
    "                continue_loop = True\n",
    "                res.extend(t)\n",
    "            else:\n",
    "                res.append(t)\n",
    "        listsOfLists = res\n",
    "    return res\n",
    "\n",
    "# generate a new sentence with tags from a structure\n",
    "\n",
    "\n",
    "def generete_sentence_from_structure(tagger: dict, gen: dict, voc: dict, structure: list) -> tuple:\n",
    "    sent_ini = init_sent(structure)\n",
    "    tagged_sentence = remove_lists(\n",
    "        [swap(tagger, gen, voc, w) for w in sent_ini])\n",
    "    sent = \" \".join([w[0] for w in tagged_sentence])\n",
    "    tags = \" \".join([w[1] for w in tagged_sentence])\n",
    "    return sent, tags\n",
    "\n",
    "# return the closest word in the vocabulary, the key and score\n",
    "\n",
    "\n",
    "def closest_word_in_voc(voc: dict, word: str) -> tuple:\n",
    "    jarowinkler = JaroWinkler()\n",
    "    sim = 0\n",
    "    for k in voc:\n",
    "        for w in voc[k]['voc']:\n",
    "            jaro = jarowinkler.similarity(w, word)\n",
    "            if jaro >= sim:\n",
    "                sim = jaro\n",
    "                result = w, k, sim\n",
    "    return result\n",
    "\n",
    "# autocorrection of a sentence based on the model and our vocabulary\n",
    "\n",
    "\n",
    "def auto_correction(model_vocab: list, voc: dict, voc_stopwords: set, sentence: str) -> str:\n",
    "    spell = SpellChecker()\n",
    "    correction = []\n",
    "    for w in sentence.split():\n",
    "        if w[0].isupper():\n",
    "            wc, wk, ws = closest_word_in_voc(voc, w)\n",
    "            if ws > 0.95:\n",
    "                correction.append(wc)\n",
    "            else:\n",
    "                if w in model_vocab or w in voc_stopwords:\n",
    "                    correction.append(w)\n",
    "                else:\n",
    "                    wc = spell.correction(w)\n",
    "                    correction.append(wc.capitalize())\n",
    "        else:\n",
    "            if w in model_vocab or w in voc_stopwords:\n",
    "                correction.append(w)\n",
    "            else:\n",
    "                wc, wk, ws = closest_word_in_voc(voc, w)\n",
    "                if ws > 0.95:\n",
    "                    correction.append(wc)\n",
    "                else:\n",
    "                    wc = spell.correction(w)\n",
    "                    correction.append(wc)\n",
    "    return \" \".join(correction)\n",
    "\n",
    "# generate and select a sentence from the gen dictionary\n",
    "\n",
    "\n",
    "def select_from_sentence_gen(gen: dict, ks: str) -> str:\n",
    "    res = ''\n",
    "    for k in ks.split():\n",
    "        if k not in gen:\n",
    "            print(\"warning :\", k, \"not found\")\n",
    "        else:\n",
    "            n = np.random.choice(len(gen[k]), 1)[0]\n",
    "            l = list(gen[k])[n].split()\n",
    "            for el in l:\n",
    "\n",
    "                if el.startswith('#'):\n",
    "                    res += select_from_sentence_gen(gen, el)\n",
    "                else:\n",
    "                    res += '_'+el\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_dataframe_for_bert(tagger: dict, \n",
    "                                gen: dict, \n",
    "                                voc: dict, \n",
    "                                structures: \n",
    "                                list, \n",
    "                                n: int = 10000) -> pd.DataFrame:\n",
    "    data_train = []\n",
    "    for i in range(n):\n",
    "        structure = structures[np.random.choice(len(structures), 1)[0]]\n",
    "        structure_init = init_structure(structure)\n",
    "        sent, tags = generete_sentence_from_structure(\n",
    "            tagger, gen, voc, structure_init)\n",
    "        for u, j in zip(sent.split(), tags.split()):\n",
    "            data_train.append([i, u, j])\n",
    "\n",
    "    train_df = pd.DataFrame(data_train, columns=[\n",
    "                            'sentence_id', 'words', 'labels'])\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that verify the integrity of the tagger dictionary\n",
    "def verif_dict_tagger(tagger: dict) -> bool:\n",
    "    verif_global = True\n",
    "    hash_keys = [k for k in tagger.keys() if k.startswith('$') == False]\n",
    "    for k in hash_keys:\n",
    "        l = tagger[k]\n",
    "        words = [w[0] for w in l]\n",
    "        tags = [w[1] for w in l]\n",
    "\n",
    "        # verif number of words = number of tags\n",
    "        left = np.array([len(words)\n",
    "                         for words in list(map(lambda x:x.split(), words))])\n",
    "        right = np.array([len(words)\n",
    "                          for words in list(map(lambda x:x.split(), tags))])\n",
    "        verif1 = all(left == right)\n",
    "        if not verif1:\n",
    "            print(k, \"\\t1-Number of words != number of tags\")\n",
    "\n",
    "        # verif all #words have #tags conterpart\n",
    "        left = np.array([words[0].startswith(\"#\")\n",
    "                         for words in list(map(lambda x:x.split(), words))\n",
    "                         if len(words) > 0])\n",
    "        right = np.array([words[0].startswith(\"#\")\n",
    "                          for words in list(map(lambda x:x.split(), tags))\n",
    "                          if len(words) > 0])\n",
    "        verif2 = all(left == right)\n",
    "        if not verif2:\n",
    "            print(k, \"\\t2-Not all words have their tags\")\n",
    "\n",
    "        # verif all links have key\n",
    "        verif3 = [m in tagger.keys() for w in words for m in w.split()\n",
    "                  if m.startswith(\"#\")]\n",
    "        if len(verif3) > 0:\n",
    "            verif3 = all(verif3)\n",
    "        else:\n",
    "            verif3 = True\n",
    "        if not verif3:\n",
    "            print(k, \"\\t3-Not all link have a key\")\n",
    "\n",
    "        verif_key = verif1 & verif2 & verif3\n",
    "        verif_global &= verif_key\n",
    "\n",
    "        if not verif_key:\n",
    "            print(k)\n",
    "\n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the generator dictionary\n",
    "\n",
    "\n",
    "def verif_dict_generator(dict_gen: dict) -> bool:\n",
    "    verif_global = True\n",
    "\n",
    "    verif_keys_in_dict = [\n",
    "        p in gen\n",
    "        for k in gen.keys()\n",
    "        for sent in list(gen[k])\n",
    "        for p in sent.split()\n",
    "        if p.startswith('#')\n",
    "    ]\n",
    "\n",
    "    verif_global &= all(verif_keys_in_dict)\n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the structures\n",
    "\n",
    "\n",
    "def verif_structure_link_tag(structure: list,\n",
    "                             tagger: dict,\n",
    "                             display: bool = False) -> bool:\n",
    "    words = list(pd.core.common.flatten([[structure]]))\n",
    "\n",
    "    verifs = [w for w in words\n",
    "              if w.startswith('#') and w not in tagger.keys()]\n",
    "\n",
    "    if display:\n",
    "        print(verifs)\n",
    "    return len(verifs) == 0\n",
    "\n",
    "# verify the integrity of the tagger in respect of the generator\n",
    "\n",
    "\n",
    "def verif_dict_tagger_links_gen(tagger: dict, gen: dict) -> bool:\n",
    "    verif_global = True\n",
    "    hash_keys = [k for k in tagger.keys()\n",
    "                 if k.startswith('$') == False]\n",
    "    for k in hash_keys:\n",
    "        l = tagger[k]\n",
    "        words = [w[0] for w in l]\n",
    "        gen_keys = [l.split(\"$GEN$\")[1] for w in words\n",
    "                    for l in w.split()\n",
    "                    if l.startswith(\"$GEN$\")]\n",
    "        verif = [gen_k in gen for gen_k in gen_keys]\n",
    "        verif = all(verif)\n",
    "\n",
    "        verif_global = verif_global & verif\n",
    "        if not verif:\n",
    "            print(k, verif)\n",
    "\n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the tagger in respect of the vocabulary\n",
    "\n",
    "\n",
    "def verif_dict_tagger_links_voc(tagger: dict, voc: dict) -> bool:\n",
    "    verif_global = True\n",
    "    hash_keys = [k for k in tagger.keys() if k.startswith('$') == False]\n",
    "    for k in hash_keys:\n",
    "        l = tagger[k]\n",
    "        words = [w[0] for w in l]\n",
    "        gen_keys = [tuple(l.split(\"$VOC$\")[1].split('#')[1:])\n",
    "                    for w in words for l in w.split()\n",
    "                    if l.startswith(\"$VOC$\")]\n",
    "\n",
    "        verif1 = all([gen_k[0] in voc for gen_k in gen_keys])\n",
    "        verif2 = all([gen_k[1] in voc[gen_k[0]] for gen_k in gen_keys])\n",
    "        verif = verif1 & verif2\n",
    "        if not verif:\n",
    "            print(k, \": a voc key is not in the voc\")\n",
    "        verif_global = verif_global & verif\n",
    "\n",
    "    return verif_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_date() -> str:\n",
    "    year = str(np.random.randint(2010, 2020)).zfill(4)\n",
    "    month = str(np.random.randint(12)).zfill(2)\n",
    "    day = str(np.random.randint(31)).zfill(2)\n",
    "    return day+month+year\n",
    "\n",
    "\n",
    "def random_timestamp() -> str:\n",
    "    year = str(np.random.randint(2010, 2020)).zfill(4)\n",
    "    month = str(np.random.randint(12)).zfill(2)\n",
    "    day = str(np.random.randint(31)).zfill(2)\n",
    "    hour = str(np.random.randint(24)).zfill(2)\n",
    "    minute = str(np.random.randint(60)).zfill(2)\n",
    "    sec = str(np.random.randint(60)).zfill(2)\n",
    "    return day+'/'+month+'/'+year+' '+hour+'h'+minute+'m'+sec+'s'\n",
    "\n",
    "\n",
    "def random_manu(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"manu\"], 1)[0]\n",
    "\n",
    "\n",
    "def random_airl(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"airl\"], 1)[0]\n",
    "\n",
    "\n",
    "def random_airc(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"airc\"], 1)[0]\n",
    "\n",
    "\n",
    "def random_cate(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"cate\"], 1)[0]\n",
    "\n",
    "\n",
    "def random_tab(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"tabs\"], 1)[0]\n",
    "\n",
    "\n",
    "def random_coun(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"coun\"], 1)[0]\n",
    "\n",
    "\n",
    "def init_filters() -> dict:\n",
    "    filters = {\n",
    "        CT_filt_manu: [],\n",
    "        CT_filt_airc: [],\n",
    "        CT_filt_airl: [],\n",
    "        CT_filt_coun: [],\n",
    "        CT_filt_cate: [],\n",
    "        CT_filt_date: [],\n",
    "    }\n",
    "    return filters\n",
    "\n",
    "\n",
    "def init_event(tab: str = \"\") -> dict:\n",
    "    event = {\n",
    "        CT_tabs: tab,\n",
    "        CT_filt: init_filters(),\n",
    "    }\n",
    "    return event\n",
    "\n",
    "\n",
    "def random_filters(prevoc: dict) -> dict:\n",
    "    filters = {\n",
    "        CT_filt_manu: [random_manu(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_airc: [random_airc(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_airl: [random_airl(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_coun: [random_coun(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_cate: [random_cate(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_date: [random_date(), random_date()],\n",
    "    }\n",
    "    return filters\n",
    "\n",
    "\n",
    "def random_event(prevoc: dict) -> dict:\n",
    "\n",
    "    event = {\n",
    "        CT_tabs: random_tab(prevoc),\n",
    "        CT_filt: random_filters(prevoc),\n",
    "    }\n",
    "    return event\n",
    "\n",
    "\n",
    "def random_session(prevoc: dict, sessid: int, n: int = None) -> pd.DataFrame:\n",
    "\n",
    "    df = make_bdd(prevoc, 0)\n",
    "    if n is None:\n",
    "        n = np.random.randint(1, 10)\n",
    "    for i in range(n):\n",
    "        timestamp = random_timestamp()\n",
    "        event = random_event(prevoc)\n",
    "        event_str = json.dumps(event)\n",
    "        row = pd.Series([str(sessid), timestamp, event_str],\n",
    "                        index=df.columns)\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_bdd(prevoc: dict, nb_session: int = 1):\n",
    "    bdd = pd.DataFrame(columns=[CT_bdd_sess, CT_bdd_date, CT_bdd_json])\n",
    "    for i in range(nb_session):\n",
    "        bdd = bdd.append(random_session(prevoc, i)).reset_index(drop=True)\n",
    "    return bdd\n",
    "\n",
    "\n",
    "def hash_event_dict(content):\n",
    "    if isinstance(content, str):\n",
    "        res = content.lower()\n",
    "\n",
    "    elif isinstance(content, int) or isinstance(content, float):\n",
    "        res = content\n",
    "\n",
    "    elif isinstance(content, list):\n",
    "        res = []\n",
    "        for ik, k in enumerate(sorted(set(content))):\n",
    "            res += [hash_event_dict(content[ik])]\n",
    "            if all([isinstance(el, str) for el in res]):\n",
    "                res = sorted(set(res))\n",
    "\n",
    "    elif isinstance(content, dict):\n",
    "        res = {}\n",
    "        for k in sorted(content.keys()):\n",
    "            res[k] = hash_event_dict(content[k])\n",
    "\n",
    "    elif isinstance(content, tuple):\n",
    "        res = tuple([])\n",
    "        for ik, k in enumerate(sorted(set(content))):\n",
    "            res += tuple([hash_event_dict(content[ik])])\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def json_string_to_hash(json_string: str) -> str:\n",
    "    event_dict = json.loads(str(json_string))\n",
    "    event_filt = {\n",
    "        key: event_dict[key]\n",
    "        for key in sorted([k.lower() for k in sorted(event_dict.keys())\n",
    "                           if k not in [CT_sess, CT_date]])\n",
    "    }\n",
    "    event_uniq = hash_event_dict(event_filt)\n",
    "    event_hash = json.dumps(event_uniq)\n",
    "    return event_hash\n",
    "\n",
    "\n",
    "def predict_next_state(state: str, df_transitions: pd.DataFrame) -> str:\n",
    "    if state in df_transitions.index:\n",
    "        pred = df_transitions.loc[state, ].idxmax()\n",
    "    else:\n",
    "        pred = np.random.choice(df_transitions.index)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfd(date: str) -> str:\n",
    "    return (date[0:2]+'-'+date[2:4]+'-'+date[4:])\n",
    "\n",
    "\n",
    "def make_sentence_fom_json(event_dict: dict) -> str:\n",
    "    sent = \"We suggest you \"\n",
    "    if event_dict[CT_tabs] == CT_tabs_default:\n",
    "        sent = sent+'the global study'\n",
    "    else:\n",
    "        sent = sent+'the '+event_dict[CT_tabs]+\"s' study\"\n",
    "    if event_dict[CT_filt][CT_date] != []:\n",
    "        sent = sent + ' from ' + \\\n",
    "            tfd(event_dict[CT_filt][CT_date][0])+' to ' + \\\n",
    "            tfd(event_dict[CT_filt][CT_date][1])\n",
    "    manu = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_manu]) == 1:\n",
    "        manu = ' for the manufacturer ' + event_dict[CT_filt][CT_filt_manu][0]\n",
    "    elif (len(event_dict[CT_filt][CT_filt_manu]) > 1):\n",
    "        manu = ' for the manufacturers '\n",
    "        for k in event_dict[CT_filt][CT_filt_manu]:\n",
    "            if k != event_dict[CT_filt][CT_filt_manu][-1]:\n",
    "                manu = manu + k+', '\n",
    "            else:\n",
    "                manu = manu + k\n",
    "\n",
    "    if manu != \"\":\n",
    "        sent = sent + manu\n",
    "    airc = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_airc]) == 1:\n",
    "        airc = ' for the aircraft' + event_dict[CT_filt][CT_filt_airc][0]\n",
    "    elif len(event_dict[CT_filt][CT_filt_airc]) > 1:\n",
    "        airc = ' for the aircrafts '\n",
    "        for k in event_dict[CT_filt][CT_filt_airc]:\n",
    "            if k != event_dict[CT_filt][CT_filt_airc][-1]:\n",
    "                airc = airc + k+', '\n",
    "            else:\n",
    "                airc = airc + k\n",
    "\n",
    "    if airc != \"\":\n",
    "        sent = sent + ' and' + airc\n",
    "    comp = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_airl]) == 1:\n",
    "        comp = ' for the company' + event_dict[CT_filt][CT_filt_airl][0]\n",
    "    elif len(event_dict[CT_filt][CT_filt_airl]) > 1:\n",
    "        comp = ' for the companies '\n",
    "        for k in event_dict[CT_filt][CT_filt_airl]:\n",
    "            if k != event_dict[CT_filt][CT_filt_airl][-1]:\n",
    "                comp = comp + k+', '\n",
    "            else:\n",
    "                comp = comp + k\n",
    "\n",
    "    if comp != \"\":\n",
    "        sent = sent + ' and'+comp\n",
    "    cat = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_cate]) == 1:\n",
    "        cat = ' and for the category' + event_dict[CT_filt][CT_filt_cate][0]\n",
    "    elif len(event_dict[CT_filt][CT_filt_cate]) > 1:\n",
    "        cat = ' and for the categories '\n",
    "        for k in event_dict[CT_filt][CT_filt_cate]:\n",
    "            if k != event_dict[CT_filt][CT_filt_cate][-1]:\n",
    "                cat = cat + k+', '\n",
    "            else:\n",
    "                cat = cat + k\n",
    "\n",
    "    if cat != \"\":\n",
    "        sent = sent + cat\n",
    "    pays = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_coun]) == 1:\n",
    "        pays = ' in ' + event_dict[CT_filt][CT_filt_coun][0]\n",
    "    elif len(event_dict[CT_filt][CT_filt_coun]) > 1:\n",
    "        pays = ' in the countries '\n",
    "        for k in event_dict[CT_filt][CT_filt_coun]:\n",
    "            if k != event_dict[CT_filt][CT_filt_coun][-1]:\n",
    "                pays = pays + k+', '\n",
    "            else:\n",
    "                pays = pays + k\n",
    "    if pays != \"\":\n",
    "        sent = sent + pays\n",
    "    sent = sent+'. If you agree, click on the following link ;)'\n",
    "    return(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
