{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(w):\n",
    "    w = w.split(',')[0].strip()\n",
    "    w = w.split('(')[0].strip()\n",
    "    w = w.split('/')[0].strip()\n",
    "    w = \"_\".join(list(map(lambda x:x.strip(), w.split('&'))))\n",
    "    w = w.replace(\"St \",\"Saint \")\n",
    "    w = w.replace(\"-\",\" \")\n",
    "    w = w.replace(\".\",\" \")\n",
    "    w = w.replace(\"*\",\" \")\n",
    "    w = w.replace(\"\\'\",\" \")\n",
    "    w = w.split()\n",
    "    if w[-1].lower() == 'airport': w = w[0:-1]\n",
    "    if w[-1].lower() == 'intl': w = w[0:-1]\n",
    "    w = \"_\".join(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the sentence generation with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_gen(model, word_list, up=False, cap=False, same=False, low=False):\n",
    "    def my_capitalize(w):\n",
    "        return \"_\".join(list(map(lambda x:x.capitalize(), w.split('_'))))\n",
    "    \n",
    "    l_upper, l_lower, l_cap, l_same = [], [], [], []\n",
    "    if up:\n",
    "        l_upper = [m.upper() for m in word_list \n",
    "                   if m.upper() in model.vocab]\n",
    "    if cap:\n",
    "        l_cap = [my_capitalize(m) \n",
    "                 for m in word_list \n",
    "                 if my_capitalize(m) in model.vocab]\n",
    "    if same:\n",
    "        l_same = [m for m in word_list \n",
    "                  if m in model.vocab]\n",
    "    if low:\n",
    "        l_lower = [m.lower() for m in word_list \n",
    "                   if m.lower() in model.vocab]\n",
    "    \n",
    "    return set(l_upper + l_cap + l_lower + l_same)\n",
    "\n",
    "# init the tags\n",
    "def init_sent(list_words):\n",
    "    res = []\n",
    "    for e in list_words:\n",
    "        if e.startswith('#'):\n",
    "            res.append((e,'#'))\n",
    "        elif e.startswith('$'):\n",
    "            res.append((e,'#'))\n",
    "        else :\n",
    "            for w in e.split():\n",
    "                res.append((w,CT_TAG_O))\n",
    "    return res\n",
    "\n",
    "# initialize a structure\n",
    "# if tuple : choose one at random\n",
    "def init_structure(struct):\n",
    "    res = []\n",
    "    if isinstance(struct, str):\n",
    "        struct = struct.strip()\n",
    "        if len(struct): res += [struct]\n",
    "    elif isinstance(struct, tuple):\n",
    "        n = np.random.choice(len(struct),1)[0]\n",
    "        l = list(struct)[n]\n",
    "        res += init_structure(l)\n",
    "    elif isinstance(struct, list):\n",
    "        for s in struct:\n",
    "            if isinstance(s, str):\n",
    "                s = s.strip()\n",
    "                if len(s) : res += [s]\n",
    "            elif isinstance(s,tuple):\n",
    "                n = np.random.choice(len(s),1)[0]\n",
    "                l = list(s)[n]\n",
    "                res += init_structure(l)\n",
    "            elif isinstance(s,list):\n",
    "                n = np.random.choice(len(s),1)[0]\n",
    "                l = list(s)[n]\n",
    "                res += init_structure(l)\n",
    "    return res\n",
    "\n",
    "# make the swaps\n",
    "def swap(tagger,W):\n",
    "    def wt_augm(w, tag):\n",
    "        ws = w.split(\"_\")\n",
    "        ws = [w for w in ws if w != '']\n",
    "        t = tag.split(CT_SEP)\n",
    "        t_name = t[0]\n",
    "        if len(t)>1:\n",
    "            t_suffix = t[-1]\n",
    "        else : t_suffix = CT_SUF_B\n",
    "        \n",
    "        tags = [t_name+CT_SEP+t_suffix if i==0 \n",
    "                else t_name+CT_SEP+CT_SUF_E if i+1==len(ws)\n",
    "                else t_name+CT_SEP+CT_SUF_I \n",
    "                for i in range(len(ws))]\n",
    "        tags = [CT_TAG_O if t.startswith(CT_TAG_O) \n",
    "                else t \n",
    "                for t in tags]\n",
    "        return list(zip(ws,tags))\n",
    "    \n",
    "    hw, tag = W\n",
    "    if hw.startswith('#'):\n",
    "        n = random.sample(range(len(tagger[hw])),1)[0]\n",
    "        w = tagger[hw][n]\n",
    "        w = list(zip(w[0].split(), w[1].split()))\n",
    "        w = [swap(tagger,(m[0], tag)) if (tag!='#')\n",
    "             else swap(tagger,(m[0], m[1])) \n",
    "             for m in w]\n",
    "        if len(w)==1: \n",
    "            w = w[0]\n",
    "            \n",
    "    elif hw.startswith('$GEN$'):\n",
    "        key = hw.split('$GEN$')[1]\n",
    "        a = select_from_sentence_gen(gen, key)[1:]\n",
    "        w = wt_augm(a, tag)\n",
    "            \n",
    "    elif hw.startswith('$VOC$'):\n",
    "        _,key,sub_key = hw.split('$VOC$')[1].split('#')\n",
    "        words_set = voc[key][sub_key]\n",
    "        n = random.sample(range(len(words_set)),1)[0]\n",
    "        w = list(words_set)[n]\n",
    "        w = wt_augm(w, tag)\n",
    "    else:\n",
    "        w = (hw, tag)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# remove list of list\n",
    "def remove_lists(listsOfLists):\n",
    "    continue_loop = True\n",
    "    while continue_loop:\n",
    "        res = []\n",
    "        continue_loop = False\n",
    "        for t in listsOfLists:\n",
    "            if isinstance(t, list):\n",
    "                continue_loop = True\n",
    "                res.extend(t)\n",
    "            else:\n",
    "                res.append(t)\n",
    "        listsOfLists = res\n",
    "    return res\n",
    "\n",
    "# generate a new sentence with tags from a structure\n",
    "def generete_sentence_from_structure(tagger, structure):\n",
    "    sent_ini = init_sent(structure)\n",
    "    tagged_sentence = remove_lists([swap(tagger,w) for w in sent_ini])\n",
    "    sent = \" \".join([w[0] for w in tagged_sentence])\n",
    "    tags = \" \".join([w[1] for w in tagged_sentence])\n",
    "    return sent, tags\n",
    "\n",
    "# return the closest word in the vocabulary, the key and score\n",
    "def closest_word_in_voc(voc, word):\n",
    "    jarowinkler = JaroWinkler()\n",
    "    sim = 0\n",
    "    for k in voc:\n",
    "        for w in voc[k]['voc']:\n",
    "            jaro = jarowinkler.similarity(w,word)\n",
    "            if jaro >= sim:\n",
    "                sim = jaro\n",
    "                result = w,k,sim\n",
    "    return result\n",
    "\n",
    "# autocorrection of a sentence based on the model and our vocabulary\n",
    "def auto_correction(model_vocab, voc, voc_stopwords, sentence):\n",
    "    spell = SpellChecker()\n",
    "    correction = []\n",
    "    for w in sentence.split():\n",
    "        if w[0].isupper():\n",
    "            wc,wk,ws = closest_word_in_voc(voc, w)\n",
    "            if ws > 0.95:\n",
    "                correction.append(wc)\n",
    "            else:\n",
    "                if w in model_vocab or w in voc_stopwords:\n",
    "                    correction.append(w)\n",
    "                else:\n",
    "                    wc = spell.correction(w)\n",
    "                    correction.append(wc.capitalize())\n",
    "        else :\n",
    "            if w in model_vocab or w in voc_stopwords:\n",
    "                correction.append(w)\n",
    "            else:\n",
    "                wc,wk,ws = closest_word_in_voc(voc, w)\n",
    "                if ws > 0.95:\n",
    "                    correction.append(wc)\n",
    "                else:\n",
    "                    wc = spell.correction(w)\n",
    "                    correction.append(wc)\n",
    "    return correction\n",
    "\n",
    "# generate and select a sentence from the gen dictionary\n",
    "def select_from_sentence_gen(gen, ks):\n",
    "    res = ''\n",
    "    for k in ks.split():\n",
    "        if k not in gen:\n",
    "            print(\"warning :\",k,\"not found\")\n",
    "        else :\n",
    "            n = np.random.choice(len(gen[k]),1)[0]\n",
    "            l = list(gen[k])[n].split()\n",
    "            for el in l:\n",
    "\n",
    "                if el.startswith('#'):\n",
    "                    res += select_from_sentence_gen(gen, el)\n",
    "                else :\n",
    "                    res += '_'+el\n",
    "    return res\n",
    "\n",
    "def generate_dataframe_for_bert(n=10000):\n",
    "    data_train = []\n",
    "    for i in range(n):\n",
    "        structure = structures[np.random.choice(len(structures),1)[0]]\n",
    "        structure_init = init_structure(structure)\n",
    "        sent,tags = generete_sentence_from_structure(tagger,structure_init)\n",
    "        for u,j in zip(sent.split(),tags.split()):\n",
    "            data_train.append([i,u,j])\n",
    "            \n",
    "    train_df = pd.DataFrame(data_train, columns=['sentence_id', 'words', 'labels'])\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that verify the integrity of the tagger dictionary\n",
    "def verif_dict_tagger(tagger):\n",
    "    verif_global = True\n",
    "    hash_keys = [k for k in tagger.keys() if k.startswith('$')==False]\n",
    "    for k in hash_keys:\n",
    "        l = tagger[k]\n",
    "        words = [w[0] for w in l]\n",
    "        tags = [w[1] for w in l]\n",
    "\n",
    "        # verif number of words = number of tags\n",
    "        left = np.array([len(words) \n",
    "                         for words in list(map(lambda x:x.split(), words))])\n",
    "        right = np.array([len(words) \n",
    "                          for words in list(map(lambda x:x.split(), tags))])\n",
    "        verif1 = all(left == right)\n",
    "        if not verif1: print(k, \"\\t1-Number of words != number of tags\")\n",
    "\n",
    "        # verif all #words have #tags conterpart\n",
    "        left = np.array([words[0].startswith(\"#\") \n",
    "                         for words in list(map(lambda x:x.split(), words))\n",
    "                         if len(words)>0])\n",
    "        right = np.array([words[0].startswith(\"#\") \n",
    "                          for words in list(map(lambda x:x.split(), tags))\n",
    "                          if len(words)>0])\n",
    "        verif2 = all(left == right)\n",
    "        if not verif2: print(k, \"\\t2-Not all words have their tags\")\n",
    "        \n",
    "        # verif all links have key\n",
    "        verif3 = [m in tagger.keys() for w in words for m in w.split()\n",
    "                  if m.startswith(\"#\")]\n",
    "        if len(verif3)>0: verif3 = all(verif3)\n",
    "        else: verif3 = True\n",
    "        if not verif3: print(k, \"\\t3-Not all link have a key\")\n",
    "\n",
    "        verif_key = verif1 & verif2 & verif3\n",
    "        verif_global &= verif_key\n",
    "\n",
    "        if not verif_key: print(k)\n",
    "    \n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the generator dictionary\n",
    "def verif_dict_generator(dict_gen):\n",
    "    verif_global = True\n",
    "    \n",
    "    verif_keys_in_dict = [\n",
    "        p in gen \n",
    "        for k in gen.keys()\n",
    "        for sent in list(gen[k])\n",
    "        for p in sent.split()\n",
    "        if p.startswith('#')\n",
    "    ]\n",
    "    \n",
    "    verif_global &= all(verif_keys_in_dict)\n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the structures\n",
    "def verif_structure_link_tag(structure, tagger, display=False):\n",
    "    words = list(pd.core.common.flatten([[structure]]))\n",
    "    \n",
    "    verifs = [w for w in words \n",
    "              if w.startswith('#') and w not in tagger.keys()]\n",
    "    \n",
    "    if display : print(verifs)\n",
    "    return len(verifs)==0\n",
    "\n",
    "# verify the integrity of the tagger in respect of the generator\n",
    "def verif_dict_tagger_links_gen(tagger,gen):\n",
    "    verif_global = True\n",
    "    hash_keys = [k for k in tagger.keys() \n",
    "                 if k.startswith('$')==False]\n",
    "    for k in hash_keys:\n",
    "        l = tagger[k]\n",
    "        words = [w[0] for w in l]\n",
    "        gen_keys = [l.split(\"$GEN$\")[1] for w in words \n",
    "                    for l in w.split() \n",
    "                    if l.startswith(\"$GEN$\")]\n",
    "        verif = [gen_k in gen for gen_k in gen_keys]\n",
    "        verif = all(verif)\n",
    "        \n",
    "        verif_global = verif_global & verif\n",
    "        if not verif: \n",
    "            print(k, verif)\n",
    "        \n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the tagger in respect of the vocabulary\n",
    "def verif_dict_tagger_links_voc(tagger,voc):\n",
    "    verif_global = True\n",
    "    hash_keys = [k for k in tagger.keys() if k.startswith('$')==False]\n",
    "    for k in hash_keys:\n",
    "        l = tagger[k]\n",
    "        words = [w[0] for w in l]\n",
    "        gen_keys = [tuple(l.split(\"$VOC$\")[1].split('#')[1:])\n",
    "                    for w in words for l in w.split() \n",
    "                    if l.startswith(\"$VOC$\")]\n",
    "        \n",
    "        verif1 = all([gen_k[0] in voc for gen_k in gen_keys])\n",
    "        verif2 = all([gen_k[1] in voc[gen_k[0]] for gen_k in gen_keys])\n",
    "        verif = verif1 & verif2\n",
    "        if not verif: print(k, \": a voc key is not in the voc\")\n",
    "        verif_global = verif_global & verif\n",
    "        \n",
    "    return verif_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_date() -> str :\n",
    "    year = str(np.random.randint(2010,2020)).zfill(4)\n",
    "    month = str(np.random.randint(12)).zfill(2)\n",
    "    day = str(np.random.randint(31)).zfill(2)\n",
    "    return day+month+year\n",
    "\n",
    "def random_timestamp() -> str :\n",
    "    year = str(np.random.randint(2010,2020)).zfill(4)\n",
    "    month = str(np.random.randint(12)).zfill(2)\n",
    "    day = str(np.random.randint(31)).zfill(2)\n",
    "    hour = str(np.random.randint(24)).zfill(2)\n",
    "    minute = str(np.random.randint(60)).zfill(2)\n",
    "    sec = str(np.random.randint(60)).zfill(2)\n",
    "    return day+'/'+month+'/'+year+' '+hour+'h'+minute+'m'+sec+'s'\n",
    "\n",
    "def random_manu(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"manu\"],1)[0]\n",
    "\n",
    "def random_airl(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"airl\"],1)[0]\n",
    "\n",
    "def random_airc(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"airc\"],1)[0]\n",
    "\n",
    "def random_cate(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"cate\"],1)[0]\n",
    "\n",
    "def random_tab(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"tabs\"],1)[0]\n",
    "\n",
    "def random_coun(prevoc) -> str:\n",
    "    return np.random.choice(prevoc[\"coun\"],1)[0]\n",
    "\n",
    "def init_filters() -> dict:\n",
    "    filters = {\n",
    "        CT_filt_manu : [],\n",
    "        CT_filt_airc : [],\n",
    "        CT_filt_airl : [],\n",
    "        CT_filt_coun : [],\n",
    "        CT_filt_cate : [],\n",
    "        CT_filt_date : [],\n",
    "    }\n",
    "    return filters\n",
    "\n",
    "def init_event() -> dict:\n",
    "    event = {\n",
    "        CT_tabs : \"\",\n",
    "        CT_filt : init_filters(),\n",
    "    }\n",
    "    return event\n",
    "\n",
    "def random_filters(prevoc) -> dict:\n",
    "    filters = {\n",
    "        CT_filt_manu : [random_manu(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_airc : [random_airc(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_airl : [random_airl(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_coun : [random_coun(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_cate : [random_cate(prevoc) for i in range(np.random.randint(5))],\n",
    "        CT_filt_date : [random_date(), random_date()],\n",
    "    }\n",
    "    return filters\n",
    "\n",
    "def random_event(prevoc) -> dict :\n",
    "\n",
    "    event = {\n",
    "        CT_tabs : random_tab(prevoc),\n",
    "        CT_filt : random_filters(prevoc),\n",
    "    }\n",
    "    return event\n",
    "\n",
    "\n",
    "def random_session(prevoc, sessid:int, n=None) -> pd.DataFrame:\n",
    "    \n",
    "    df = make_bdd(prevoc, 0)\n",
    "    if n is None:\n",
    "        n = np.random.randint(1,10)\n",
    "    for i in range(n):\n",
    "        timestamp = random_timestamp()\n",
    "        event = random_event(prevoc)\n",
    "        event_str = json.dumps(event)\n",
    "        row = pd.Series([str(sessid), timestamp, event_str],\n",
    "                        index = df.columns)\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def make_bdd(prevoc, nb_session : int = 1):\n",
    "    bdd = pd.DataFrame(columns=[CT_bdd_sess, CT_bdd_date, CT_bdd_json])\n",
    "    for i in range(nb_session):\n",
    "        bdd = bdd.append(random_session(prevoc, i)).reset_index(drop=True)\n",
    "    return bdd\n",
    "\n",
    "def hash_event_dict(content):\n",
    "    if isinstance(content, str):\n",
    "        res = content.lower()\n",
    "    \n",
    "    elif isinstance(content, int) or isinstance(content, float):\n",
    "        res = content\n",
    "    \n",
    "    elif isinstance(content, list):\n",
    "        res = []\n",
    "        content = sorted(set(content))\n",
    "        for ik,k in enumerate(content):\n",
    "            res += [hash_event_dict(content[ik])]\n",
    "    \n",
    "    elif isinstance(content, dict):\n",
    "        res = {}\n",
    "        for k in content.keys():\n",
    "            res[k] = hash_event_dict(content[k])\n",
    "    \n",
    "    elif isinstance(content, tuple):\n",
    "        res = tuple([])\n",
    "        content = sorted(set(content))\n",
    "        for ik,k in enumerate(content):\n",
    "            res += tuple([hash_event_dict(content[ik])])\n",
    "        \n",
    "    return res\n",
    "\n",
    "def json_string_to_hash(json_string):\n",
    "    event_dict = json.loads(str(json_string))\n",
    "    event_dict_to_hash = {\n",
    "        key : event_dict[key] \n",
    "        for key in sorted([k for k in event_dict.keys()\n",
    "                           if k not in [CT_sess, CT_date]])\n",
    "    }\n",
    "    event_dict_filtered = hash_event_dict(event_dict_to_hash)\n",
    "    event_dict_hasheded = json.dumps(event_dict_filtered)\n",
    "    return event_dict_hasheded\n",
    "\n",
    "def predict_next_state(state, df_transitions):\n",
    "    return df_transitions.loc[state,].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfd (date):\n",
    "    return (date[0:2]+'-'+date[2:4]+'-'+date[4:])\n",
    "\n",
    "def make_sentence_fom_json(event_dict):\n",
    "    sent = \"We suggest you \"\n",
    "    if event_dict[CT_tabs] == CT_tabs_gen:\n",
    "        sent = sent+'the global study'\n",
    "    else:\n",
    "        sent = sent+'the'+event_dict[CT_tabs]+\"s' study\"\n",
    "    sent = sent + ' from '+tfd(event_dict[CT_filt][CT_date][0])+' to '+tfd(event_dict[CT_filt][CT_date][1])\n",
    "    manu = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_manu]) == 1:\n",
    "        manu = ' for the manufacturer'+ event_dict[CT_filt][CT_filt_manu][0]\n",
    "    elif (len(event_dict[CT_filt][CT_filt_manu]) > 1):\n",
    "        manu = ' for the manufacturers '\n",
    "        for k in event_dict[CT_filt][CT_filt_manu]:\n",
    "            if k != event_dict[CT_filt][CT_filt_manu][-1]:\n",
    "                manu = manu + k+','\n",
    "            else: \n",
    "                manu = manu + k\n",
    "            \n",
    "    if manu == \"\":\n",
    "        sent = sent + ' for'\n",
    "    else:\n",
    "        sent = sent + manu + ' and'\n",
    "    airc = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_airc]) == 1:\n",
    "        airc = ' for the aircraft'+ event_dict[CT_filt][CT_filt_airc][0]\n",
    "    elif len(event_dict[CT_filt][CT_filt_airc]) > 1:\n",
    "        airc = ' for the aircrafts '\n",
    "        for k in event_dict[CT_filt][CT_filt_airc]:\n",
    "            if k != event_dict[CT_filt][CT_filt_airc][-1]:\n",
    "                airc = airc + k+','\n",
    "            else: \n",
    "                airc = airc + k\n",
    "            \n",
    "    if airc == \"\":\n",
    "        sent = sent + ' for'\n",
    "    else:\n",
    "        sent = sent + airc + ' and'\n",
    "    comp = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_airl]) == 1:\n",
    "        comp = ' for the company'+ event_dict[CT_filt][CT_filt_airl][0]\n",
    "    elif len(event_dict[CT_filt][CT_filt_airl]) > 1:\n",
    "        comp = ' for the companies '\n",
    "        for k in event_dict[CT_filt][CT_filt_airl]:\n",
    "            if k != event_dict[CT_filt][CT_filt_airl][-1]:\n",
    "                comp = comp + k+','\n",
    "            else: \n",
    "                comp = comp + k\n",
    "            \n",
    "    if comp == \"\":\n",
    "        sent = sent + ' for'\n",
    "    else:\n",
    "        sent = sent + comp + ' and'\n",
    "    cat = ''\n",
    "    if len(event_dict[CT_filt][CT_filt_cate]) == 1:\n",
    "        cat = ' for the category'+ event_dict[CT_filt][CT_filt_cate][0]\n",
    "    elif len(event_dict[CT_filt][CT_filt_cate]) > 1:\n",
    "        cat = ' for the categories '\n",
    "        for k in event_dict[CT_filt][CT_filt_cate]:\n",
    "            if k != event_dict[CT_filt][CT_filt_cate][-1]:\n",
    "                 cat = cat + k+' , '\n",
    "            else: \n",
    "                 cat = cat + k\n",
    "            \n",
    "    if cat == \"\":\n",
    "        sent = sent \n",
    "    else:\n",
    "        sent = sent + cat\n",
    "    sent = sent+'. If you agree, click onto the following link ;)'  \n",
    "    return(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
