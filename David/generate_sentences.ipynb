{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from strsimpy.jaro_winkler import JaroWinkler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathword2vec = './data/GoogleNews-vectors-negative300-SLIM.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(pathword2vec, binary=True)\n",
    "voc_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aircraft = pd.read_csv('./data/df_aircraft.csv', sep='ยง', engine='python', index_col=0, encoding='utf-8')\n",
    "df_airline = pd.read_csv('./data/df_airline.csv', sep='ยง', engine='python', index_col=0, encoding='utf-8')\n",
    "df_airport = pd.read_csv('./data/df_airport.csv', sep='ยง', engine='python', index_col=0, encoding='utf-8')\n",
    "df_country = pd.read_csv('./data/df_country.csv', sep='ยง', engine='python', index_col=0, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(w):\n",
    "    w = w.split(',')[0].strip()\n",
    "    w = w.split('(')[0].strip()\n",
    "    w = w.split('/')[0].strip()\n",
    "    w = \"_\".join(list(map(lambda x:x.strip(), w.split('&'))))\n",
    "    w = w.replace(\"St \",\"Saint \")\n",
    "    w = w.replace(\"-\",\" \")\n",
    "    w = w.replace(\".\",\" \")\n",
    "    w = w.replace(\"*\",\" \")\n",
    "    w = w.replace(\"\\'\",\" \")\n",
    "    w = w.split()\n",
    "    if w[-1].lower() == 'airport': w = w[0:-1]\n",
    "    if w[-1].lower() == 'intl': w = w[0:-1]\n",
    "    w = \"_\".join(w)\n",
    "    return w\n",
    "\n",
    "def word_gen(model, word_list, up=False, cap=False, same=False, low=False):\n",
    "    def my_capitalize(w):\n",
    "        return \"_\".join(list(map(lambda x:x.capitalize(), w.split('_'))))\n",
    "    \n",
    "    l_upper, l_lower, l_cap, l_same = [], [], [], []\n",
    "    if up:\n",
    "        l_upper = [m.upper() for m in word_list if m.upper() in model.vocab]\n",
    "    if cap:\n",
    "        l_cap = [my_capitalize(m) for m in word_list if my_capitalize(m) in model.vocab]\n",
    "    if same:\n",
    "        l_same = [m for m in word_list if m in model.vocab]\n",
    "    if low:\n",
    "        l_lower = [m.lower() for m in word_list if m.lower() in model.vocab]\n",
    "    \n",
    "    return set(l_upper + l_cap + l_lower + l_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the tags\n",
    "def init_sent(list_words):\n",
    "    res = []\n",
    "    for e in list_words:\n",
    "        if e.startswith('#'):\n",
    "            res.append((e,'#'))\n",
    "        elif e.startswith('$'):\n",
    "            res.append((e,'#'))\n",
    "        else :\n",
    "            for w in e.split():\n",
    "                res.append((w,'0'))\n",
    "    return res\n",
    "\n",
    "# make the swaps\n",
    "def swap(tagger,W):\n",
    "    def wt_augm(w, tag):\n",
    "        ws = w.split(\"_\")\n",
    "        ws = [w for w in ws if w != '']\n",
    "        t = tag.split(CT_SEP)\n",
    "        t_name = t[0]\n",
    "        if len(t)>1:\n",
    "            t_suffix = t[-1]\n",
    "        else : t_suffix = CT_SUF_B\n",
    "        \n",
    "        tags = [t_name+CT_SEP+t_suffix if i==0 \n",
    "                else t_name+CT_SEP+CT_SUF_E if i+1==len(ws)\n",
    "                else t_name+CT_SEP+CT_SUF_I \n",
    "                for i in range(len(ws))]\n",
    "        tags = ['0' if t.startswith('0') else t for t in tags]\n",
    "        return list(zip(ws,tags))\n",
    "    \n",
    "    hw, tag = W\n",
    "    if hw.startswith('#'):\n",
    "        n = random.sample(range(len(tagger[hw])),1)[0]\n",
    "        w = tagger[hw][n]\n",
    "        w = list(zip(w[0].split(), w[1].split()))\n",
    "        w = [swap(tagger,(m[0], tag)) if (tag!='#')\n",
    "             else swap(tagger,(m[0], m[1])) \n",
    "             for m in w]\n",
    "        if len(w)==1: \n",
    "            w = w[0]\n",
    "            \n",
    "    elif hw.startswith('$GEN$'):\n",
    "        key = hw.split('$GEN$')[1]\n",
    "        a = select_from_sentence_gen(gen, key)[1:]\n",
    "        w = wt_augm(a, tag)\n",
    "            \n",
    "    elif hw.startswith('$'):\n",
    "            n = random.sample(range(len(tagger[hw])),1)[0]\n",
    "            w = list(tagger[hw])[n]\n",
    "            w = wt_augm(w, tag)\n",
    "    else:\n",
    "        w = (hw, tag)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# remove list of list\n",
    "def remove_lists(listsOfLists):\n",
    "    continue_loop = True\n",
    "    while continue_loop:\n",
    "        res = []\n",
    "        continue_loop = False\n",
    "        for t in listsOfLists:\n",
    "            if isinstance(t, list):\n",
    "                continue_loop = True\n",
    "                res.extend(t)\n",
    "            else:\n",
    "                res.append(t)\n",
    "        listsOfLists = res\n",
    "    return res\n",
    "\n",
    "# generate a new sentence with tags from a structure\n",
    "def generete_sentence_from_structure(tagger, structure):\n",
    "    sent_ini = init_sent(structure)\n",
    "    tagged_sentence = remove_lists([swap(tagger,w) for w in sent_ini])\n",
    "    sent = \" \".join([w[0] for w in tagged_sentence])\n",
    "    tags = \" \".join([w[1] for w in tagged_sentence])\n",
    "    return sent, tags\n",
    "\n",
    "# return the closest word in the vocabulary, the key and score\n",
    "def closest_word_in_voc(voc, word):\n",
    "    jarowinkler = JaroWinkler()\n",
    "    sim = 0\n",
    "    for k in voc:\n",
    "        for w in voc[k]['voc']:\n",
    "            jaro = jarowinkler.similarity(w,word)\n",
    "            if jaro >= sim:\n",
    "                sim = jaro\n",
    "                result = w,k,sim\n",
    "    return result\n",
    "\n",
    "# autocorrection of a sentence based on the model and our vocabulary\n",
    "def auto_correction(model_vocab, voc, voc_stopwords, sentence):\n",
    "    spell = SpellChecker()\n",
    "    correction = []\n",
    "    for w in sentence.split():\n",
    "        if w[0].isupper():\n",
    "            wc,wk,ws = closest_word_in_voc(voc, w)\n",
    "            if ws > 0.95:\n",
    "                correction.append(wc)\n",
    "            else:\n",
    "                if w in model_vocab or w in voc_stopwords:\n",
    "                    correction.append(w)\n",
    "                else:\n",
    "                    wc = spell.correction(w)\n",
    "                    correction.append(wc.capitalize())\n",
    "        else :\n",
    "            if w in model_vocab or w in voc_stopwords:\n",
    "                correction.append(w)\n",
    "            else:\n",
    "                wc,wk,ws = closest_word_in_voc(voc, w)\n",
    "                if ws > 0.95:\n",
    "                    correction.append(wc)\n",
    "                else:\n",
    "                    wc = spell.correction(w)\n",
    "                    correction.append(wc)\n",
    "    return correction\n",
    "\n",
    "# generate and select a sentence from the gen dictionary\n",
    "def select_from_sentence_gen(gen, ks):\n",
    "    res = ''\n",
    "    for k in ks.split():\n",
    "        if k not in gen:\n",
    "            print(\"warning :\",k,\"not found\")\n",
    "        else :\n",
    "            n = np.random.choice(len(gen[k]),1)[0]\n",
    "            l = list(gen[k])[n].split()\n",
    "            for el in l:\n",
    "\n",
    "                if el.startswith('#'):\n",
    "                    res += select_from_sentence_gen(gen, el)\n",
    "                else :\n",
    "                    res += '_'+el\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that verify the integrity of the tagger dictionary\n",
    "def verif_dict_tagger(dict_swaps):\n",
    "    verif_global = True\n",
    "    hash_keys = [k for k in dict_swaps.keys() if k.startswith('$')==False]\n",
    "    for k in hash_keys:\n",
    "        l = dict_swaps[k]\n",
    "        words = [w[0] for w in l]\n",
    "        tags = [w[1] for w in l]\n",
    "\n",
    "        # verif number of words = number of tags\n",
    "        left = np.array([len(words) for words in list(map(lambda x:x.split(), words))])\n",
    "        right = np.array([len(words) for words in list(map(lambda x:x.split(), tags))])\n",
    "        verif1 = all(left == right)\n",
    "        if not verif1: print(k, \"\\t1-Number of words != number of tags\")\n",
    "\n",
    "        # verif all #words have #tags conterpart\n",
    "        left = np.array([words[0].startswith(\"#\") \n",
    "                         for words in list(map(lambda x:x.split(), words))\n",
    "                         if len(words)>0])\n",
    "        right = np.array([words[0].startswith(\"#\") \n",
    "                          for words in list(map(lambda x:x.split(), tags))\n",
    "                          if len(words)>0])\n",
    "        verif2 = all(left == right)\n",
    "        if not verif2: print(k, \"\\t2-Not all words have their tags\")\n",
    "        \n",
    "        # verif all links have key\n",
    "        verif3 = [m in dict_swaps.keys() for w in words for m in w.split()\n",
    "                  if m.startswith(\"#\") or (m.startswith(\"$GEN$\")==False and m.startswith(\"$\"))]\n",
    "        if len(verif3)>0: verif3 = all(verif3)\n",
    "        else: verif3 = True\n",
    "        if not verif3: print(k, \"\\t3-Not all link have a key\")\n",
    "\n",
    "        verif_key = verif1 & verif2 & verif3\n",
    "        verif_global &= verif_key\n",
    "\n",
    "        if not verif_key: print(k)\n",
    "    \n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the generator dictionary\n",
    "def verif_dict_generator(dict_gen):\n",
    "    verif_global = True\n",
    "    \n",
    "    verif_keys_in_dict = [\n",
    "        p in gen \n",
    "        for k in gen.keys()\n",
    "        for sent in list(gen[k])\n",
    "        for p in sent.split()\n",
    "        if p.startswith('#')\n",
    "    ]\n",
    "    \n",
    "    verif_global &= all(verif_keys_in_dict)\n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the structures\n",
    "def verif_structures(structures, tagger, display=False):\n",
    "    verif_global = True\n",
    "    \n",
    "    verif_keys_in_dict = [\n",
    "        all([word in tagger for word in struct if word.startswith('#')])\n",
    "        for struct in structures\n",
    "    ]\n",
    "    if display: print(verif_keys_in_dict)\n",
    "    \n",
    "    verif_global = verif_global & all(verif_keys_in_dict)\n",
    "    return verif_global\n",
    "\n",
    "# verify the integrity of the tagger in respect of the generator\n",
    "def verif_dict_tagger_links_gen(tagger,gen):\n",
    "    verif_global = True\n",
    "    hash_keys = [k for k in tagger.keys() if k.startswith('$')==False]\n",
    "    for k in hash_keys:\n",
    "        l = tagger[k]\n",
    "        words = [w[0] for w in l]\n",
    "        gen_keys = [l.split(\"$GEN$\")[1] for w in words for l in w.split() if l.startswith(\"$GEN$\")]\n",
    "        verif = [gen_k in gen for gen_k in gen_keys]\n",
    "        verif = all(verif)\n",
    "        \n",
    "        verif_global = verif_global & verif\n",
    "        if not verif: \n",
    "            print(k, verif)\n",
    "        \n",
    "    return verif_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "manu = ['Airbus','Boeing','Cessna','Cirrus ','Pilatus','Matra']\n",
    "coun = list(df_country['country'].apply(preprocess))\n",
    "citi = list(df_airport['location'].apply(preprocess))\n",
    "airp = list(df_airport['airport'].apply(preprocess))\n",
    "airl = list(df_airline['airline'].apply(preprocess))\n",
    "mont = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "seas = ['Winter','Spring','Autumn','Winter']\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "year = ['Year','Years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_SEP = '_'\n",
    "\n",
    "CT_SUF_B='B'\n",
    "CT_SUF_I='I'\n",
    "CT_SUF_E='E'\n",
    "\n",
    "CT_TAG_STAT = 'STAT'\n",
    "CT_TAG_MANU = 'MANU'\n",
    "CT_TAG_AIRP = 'AIRP'\n",
    "CT_TAG_AIRL = 'AIRL'\n",
    "CT_TAG_COUN = 'COUN'\n",
    "CT_TAG_DATE1 = 'DATE1'\n",
    "CT_TAG_DATE2 = 'DATE2'\n",
    "CT_TAG_STUD = 'STUDIED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = {\n",
    "    \"manu\" : {'voc' : word_gen(model, manu, up=True, cap=True, same=True, low=False),\n",
    "              'tag' : CT_TAG_MANU,\n",
    "              'name' : 'Airplane Manufacturer'},\n",
    "    \"coun\" : {'voc' : word_gen(model, coun, up=True, cap=True, same=True, low=False),\n",
    "              'tag' : CT_TAG_COUN,\n",
    "              'name' : 'Country'},\n",
    "    \"citi\" : {'voc' : word_gen(model, citi, up=True, cap=True, same=True, low=False),\n",
    "              'tag' : None,\n",
    "              'name' : 'City'},\n",
    "    \"airp\" : {'voc' : word_gen(model, airp, up=True, cap=True, same=True, low=False),\n",
    "              'tag' : CT_TAG_AIRP,\n",
    "              'name' : 'Airport'},\n",
    "    \"airl\" : {'voc' : word_gen(model, airl, up=True, cap=True, same=True, low=False),\n",
    "              'tag' : CT_TAG_AIRL,\n",
    "              'name' : 'Airline'},\n",
    "    \"mont\" : {'voc' : word_gen(model, mont, up=True, cap=True, same=True, low=False),\n",
    "              'tag' : None,\n",
    "              'name' : 'Month'},\n",
    "    \"seas\" : {'voc' : word_gen(model, seas, up=True, cap=True, same=True, low=True),\n",
    "              'tag' : None,\n",
    "              'name' : 'Season'},\n",
    "    \"days\" : {'voc' : word_gen(model, days, up=True, cap=True, same=True, low=True),\n",
    "              'tag' : None,\n",
    "              'name' : 'Days'},\n",
    "    \"year\" : {'voc' : word_gen(model, year, up=True, cap=True, same=True, low=False),\n",
    "              'tag' : None,\n",
    "              'name' : 'Year'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = {\n",
    "    '#client' : {\n",
    "        'client','clients','customer','customers','passenger','passengers',\n",
    "    },\n",
    "    '#satisfaction' : {\n",
    "        'contentment','contentments','satisfaction','satisfactions',\n",
    "    },\n",
    "    '#quantity' : {\n",
    "        'amount','amounts','number','quantity','sum','sums',\n",
    "    },\n",
    "    '#flight' : {\n",
    "        'flight','flights','travel','travels',\n",
    "    },\n",
    "    '#client_satisfaction' : {\n",
    "        '#client #satisfaction',\n",
    "        '#satisfaction of #client'\n",
    "    },\n",
    "    '#quantity_of_flights' : {\n",
    "        '#flight #quantity',\n",
    "        '#quantity of #flight'\n",
    "    },\n",
    "    '#PROP#to' : {\n",
    "        'to','to the beginning of','to the end of','until',\n",
    "    },\n",
    "    '#PROP#from' : {\n",
    "        'from','from the beginning of','from the end of','in',\n",
    "    },\n",
    "    '#STATS#graph' : {\n",
    "        'chart','charts','graph','graphs','histogram','histograms','pie chart','pie charts',\n",
    "        'slope','slopes',\n",
    "    },\n",
    "    '#VERB#show' : {\n",
    "        'display','highlight','plot','print','show','view',\n",
    "    },\n",
    "    '#VERB#be' : {\n",
    "        'happen to be','is','seems','seems to be', \n",
    "    },\n",
    "    '#VERB#like' : {\n",
    "        'like','likes','love','loves',\n",
    "    },\n",
    "    '#VERB#dislike' : {\n",
    "        'cannot stand','dislike','dislikes','hate','hates',\n",
    "    },\n",
    "    '#PRONON#meus' : {\n",
    "        'me','us','',\n",
    "    },\n",
    "    '#ARTICLE#' : {\n",
    "        '','a','an','the',\n",
    "    },\n",
    "    '#COMP#good' : {\n",
    "        'able', 'acceptable', 'ace', 'admirable', 'advantageous', 'agreeable', 'amazing', 'appropriate', \n",
    "        'awesome', 'benefic', 'capable', 'capital', 'clever', 'comfortable', 'commendable', 'common', \n",
    "        'congenial', 'convenient', 'decent', 'deluxe', 'efficient', 'excellent', 'exceptional', \n",
    "        'expert', 'fascinating', 'favorable', 'first-class', 'first-rate', 'flawless', 'fresh', \n",
    "        'friendly', 'good', 'gratifying', 'great', 'healthy', 'helpful', 'honest', 'honorable', \n",
    "        'hygienic', 'incredible', 'intact', 'kindhearted', 'marvelous', 'neat', 'nice', 'normal', \n",
    "        'opportune', 'perfect', 'pleasant', 'pleasing', 'positive', 'precious', 'prime', 'prodigious', \n",
    "        'profitable', 'qualified', 'rad', 'reliable', 'reputable', 'respectable', 'right', 'safe', \n",
    "        'salutary', 'satisfactory', 'satisfying', 'serviceable', 'shipshape', 'shocking', 'skillful',\n",
    "        'solid', 'splendid', 'stable', 'sterling', 'stunning', 'stupendous', 'suitable', 'suited', \n",
    "        'super', 'superb', 'superior', 'surprising', 'talented', 'tasty', 'tip-top', 'tolerable', \n",
    "        'trustworthy', 'unbelievable', 'useful', 'valuable', 'welcome', 'wonderful', 'worthy',\n",
    "    },\n",
    "    '#COMP#bad' : {\n",
    "        'abominable', 'amiss', 'atrocious', 'awful', 'bad', 'bummer', 'careless', 'catastrophic', \n",
    "        'chaotic', 'cheap', 'cheesy', 'crap', 'crappy', 'crummy', 'damaging', 'dangerous', 'defective', \n",
    "        'deficient', 'deleterious', 'detrimental', 'disagreeable', 'disastrous', 'discouraging', \n",
    "        'displeasing', 'distressing', 'dreadful', 'dumb', 'erroneous', 'evil', 'fallacious', \n",
    "        'garbage', 'godawful', 'grim', 'grody', 'gross', 'grungy', 'harsh', 'hurtful', 'icky', \n",
    "        'imperfect', 'impolite', 'inadequate', 'incorrect', 'iniquitous', 'injurious', 'junky', \n",
    "        'lame', 'loud', 'lousy', 'mean', 'moldy', 'noisy', 'not good', 'old', 'painful', 'poor', \n",
    "        'rancid', 'regretful', 'rotten', 'rude', 'ruinous', 'sad', 'shitty', 'slipshod', 'spoiled', \n",
    "        'stinking', 'strident', 'substandard', 'terrible', 'tragic', 'troubled', 'troubling', \n",
    "        'unacceptable', 'unfavorable', 'unfortunate', 'unhappy', 'unhealthy', 'unlucky', 'unpleasant', \n",
    "        'unsatisfactory', 'unwell', 'upsetting', 'vicious', 'wicked', 'wrong',\n",
    "    },\n",
    "}\n",
    "\n",
    "verif_dict_generator(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fresh_clients_satisfactions'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_from_sentence_gen(gen, \"#COMP#good #client_satisfaction\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tagger = {\n",
    "    '#STATS#graph' : [('$GEN$#STATS#graph',CT_TAG_STAT)],\n",
    "    '#VERB#show' : [('$GEN$#VERB#show','0')],\n",
    "    '#PRONON#meus' : [('$GEN$#PRONON#meus','0')],\n",
    "    '#ARTICLE#' : [('$GEN$#ARTICLE#','0')],\n",
    "\n",
    "    # Named Entity\n",
    "    '#NE#manu' : [\n",
    "        ('$manu$voc',CT_TAG_MANU),\n",
    "    ],\n",
    "\n",
    "    '#NE#airp' : [\n",
    "        ('$airp$voc',CT_TAG_AIRP),\n",
    "        ('$airp$voc airport',CT_TAG_AIRP+' 0'),\n",
    "        ('airport of $airp$voc','0 0 '+CT_TAG_AIRP),\n",
    "    ],\n",
    "\n",
    "    '#NE#airl' : [\n",
    "        ('$airl$voc',CT_TAG_AIRL),\n",
    "    ],\n",
    "\n",
    "    '#NE#coun' : [\n",
    "        ('$coun$voc',CT_TAG_COUN),\n",
    "    ],\n",
    "\n",
    "    # Date1\n",
    "    '#DATE1#' : [\n",
    "      ('#DATE1#y','#'),\n",
    "      ('the year #DATE1#y','0 0 #'),\n",
    "      ('year #DATE1#y','0 #'),\n",
    "      ('#DATE1#my','#'),\n",
    "      ('#DATE1#sy','#'),\n",
    "    ],\n",
    "    '#DATE1#y' : [\n",
    "      ('$year$voc',\n",
    "       CT_TAG_DATE1+CT_SEP+CT_SUF_B),\n",
    "    ],\n",
    "    '#DATE1#my' : [\n",
    "      ('$mont$voc $year$voc',\n",
    "       CT_TAG_DATE1+CT_SEP+CT_SUF_B+' '+\\\n",
    "       CT_TAG_DATE1+CT_SEP+CT_SUF_E),\n",
    "    ],\n",
    "    '#DATE1#sy' : [\n",
    "      ('$seas$voc $year$voc',\n",
    "       CT_TAG_DATE1+CT_SEP+CT_SUF_B+' '+\\\n",
    "       CT_TAG_DATE1+CT_SEP+CT_SUF_E),\n",
    "    ],\n",
    "\n",
    "    # Date2\n",
    "    '#DATE2#' : [\n",
    "      ('#DATE2#y','#'),\n",
    "      ('the year #DATE2#y','0 0 #'),\n",
    "      ('year #DATE2#y','0 #'),\n",
    "      ('#DATE2#my','#'),\n",
    "      ('#DATE2#sy','#'),\n",
    "    ],\n",
    "    '#DATE2#y' : [\n",
    "      ('$year$voc',\n",
    "       CT_TAG_DATE2+CT_SEP+CT_SUF_B)\n",
    "    ],\n",
    "    '#DATE2#my' : [\n",
    "      ('$mont$voc $year$voc',\n",
    "       CT_TAG_DATE2+CT_SEP+CT_SUF_B+' '+\\\n",
    "       CT_TAG_DATE2+CT_SEP+CT_SUF_E),\n",
    "    ],\n",
    "    '#DATE2#sy' : [\n",
    "      ('$seas$voc $year$voc',\n",
    "       CT_TAG_DATE2+CT_SEP+CT_SUF_B+' '+\\\n",
    "       CT_TAG_DATE2+CT_SEP+CT_SUF_E),\n",
    "    ],\n",
    "\n",
    "    # Studied variable\n",
    "    '#STUDIED#' : [\n",
    "      ('#ARTICLE# $GEN$#quantity_of_flights','# '+CT_TAG_STUD),\n",
    "      ('#ARTICLE# $GEN$#client_satisfaction','# '+CT_TAG_STUD),\n",
    "    ],\n",
    "\n",
    "    # Propositions SHOW\n",
    "    '#PROP#show_meus_the' : [\n",
    "      ('#VERB#show #PRONON#meus #ARTICLE#','# # #')\n",
    "    ],\n",
    "\n",
    "    # Propositions DATES\n",
    "    '#PROP#DATE#from_to' : [\n",
    "      ('$GEN$#PROP#from #DATE1# $GEN$#PROP#to #DATE2#','0 # 0 #'),\n",
    "    ],\n",
    "    '#PROP#DATE#for' : [\n",
    "      ('for #ARTICLE# #DATE1#','0 # #'),\n",
    "    ],\n",
    "    '#PROP#DATE#since' : [\n",
    "      ('since #ARTICLE# #DATE1#','0 # #'),\n",
    "    ],\n",
    "    \n",
    "    # SELECTORS\n",
    "    '$manu$voc' : voc['manu']['voc'],\n",
    "    '$coun$voc' : voc['coun']['voc'],\n",
    "    '$airl$voc' : voc['airl']['voc'],\n",
    "    '$airp$voc' : voc['airp']['voc'],\n",
    "    '$airl$voc' : voc['airl']['voc'],\n",
    "    '$mont$voc' : voc['mont']['voc'],\n",
    "    '$seas$voc' : voc['seas']['voc'],\n",
    "    '$days$voc' : voc['days']['voc'],\n",
    "    '$year$voc' : voc['year']['voc'],\n",
    "}\n",
    "\n",
    "print(verif_dict_tagger(tagger))\n",
    "print(verif_dict_tagger_links_gen(tagger,gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures = [\n",
    "    ['#PROP#show_meus_the','#STATS#graph','of', '#STUDIED#','for','#NE#manu', 'and',\n",
    "     \"#NE#manu\", 'in', '#NE#coun', \"#PROP#DATE#from_to\"\n",
    "    ],\n",
    "    ['#PROP#show_meus_the','#STATS#graph','of', '#STUDIED#','for','#NE#manu', 'and',\n",
    "     \"#NE#manu\", 'in', '#NE#coun', \"#PROP#DATE#for\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "verif_structures(structures, tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same length : True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('view a graph of a client contentments for MATRA and Pilatus in Guatemala for year YEAR',\n",
       " '0 0 STAT_B 0 0 STUDIED_B STUDIED_E 0 MANU_B 0 MANU_B 0 COUN_B 0 0 DATE1_B')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = structures[np.random.choice(len(structures),1)[0]]\n",
    "sent,tags = generete_sentence_from_structure(tagger,structure)\n",
    "print(\"Same length :\", len(sent.split()) == len(tags.split()))\n",
    "sent,tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'would',\n",
       " 'like',\n",
       " 'the',\n",
       " 'graph',\n",
       " 'of',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'flight',\n",
       " 'for',\n",
       " 'Boeing',\n",
       " 'on',\n",
       " 'monday',\n",
       " '2020']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'I woulde lirke the graph of the numbr of fligt for Boing on mondy 2020'\n",
    "auto_correction(model.vocab, voc, voc_stopwords, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
